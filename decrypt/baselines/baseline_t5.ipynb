{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## For running the T5 vanilla baseline\n",
    "Here we provide code to\n",
    "1. Setup datafiles for running t5 (i.e. produce json files)\n",
    "1. Run the seq2seq model to produce outputs, saving in a directory that matches k_t5_outputs below\n",
    "    - Train model (produces saved checkpoints)\n",
    "    - Eval top performing model (load from top checkpoint, produces json outputs)\n",
    "\n",
    "1. run eval code using the json output from model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt/baselines', '/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python310.zip', '/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10', '/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/lib-dynload', '', '/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages', '/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/bitsandbytes-0.41.1-py3.10.egg', '/home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt', '/home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt', '/home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "path = str(Path(Path.cwd()).parent.absolute())\n",
    "sys.path.append(path) \n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from decrypt.scrape_parse import (\n",
    "    load_guardian_splits,\n",
    "    load_guardian_splits_disjoint,\n",
    "    load_guardian_splits_disjoint_hash\n",
    ")\n",
    "\n",
    "import os\n",
    "from decrypt import config\n",
    "from decrypt.common import validation_tools as vt\n",
    "from decrypt.common.util_data import clue_list_tuple_to_train_split_json\n",
    "import logging\n",
    "logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "k_json_folder = config.DataDirs.Guardian.json_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Produce datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:decrypt.scrape_parse.guardian_load:Loading splits directly from given json files. Using /home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt/data/naive_random.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142380/142380 [00:00<00:00, 2194653.63it/s]\n",
      "INFO:decrypt.scrape_parse.guardian_load:Counter({1: 118540, 2: 20105, 3: 2929, 4: 686, 5: 112, 6: 8})\n",
      "INFO:decrypt.scrape_parse.guardian_load:Clue list length matches Decrypting paper expected length\n",
      "INFO:decrypt.scrape_parse.guardian_load:Got splits of lenghts [85428, 28476, 28476]\n",
      "INFO:decrypt.scrape_parse.guardian_load:First three clues of train set:\n",
      "\t[CleanGuardianClue(clue='Suffering to grasp edge of plant', lengths=[8], soln='agrimony', soln_with_spaces='agrimony', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Chifonie', orig_lengths='8', lengths_punctuation=set()), CleanGuardianClue(clue='Honour Ben and Noel with new order', lengths=[7], soln='ennoble', soln_with_spaces='ennoble', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Rufus', orig_lengths='7', lengths_punctuation=set()), CleanGuardianClue(clue='Bit the royal we love? Cheers!', lengths=[4], soln='iota', soln_with_spaces='iota', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Screw', orig_lengths='4', lengths_punctuation=set())]\n",
      "100%|██████████| 85428/85428 [00:00<00:00, 782634.06it/s]\n",
      "100%|██████████| 28476/28476 [00:00<00:00, 731293.21it/s]\n",
      "100%|██████████| 28476/28476 [00:00<00:00, 629649.74it/s]\n",
      "INFO:decrypt.common.util_data:Source target mapping:\n",
      "\tSuffering to grasp edge of plant (8) => agrimony\n",
      "\n",
      "INFO:decrypt.common.util_data:Finished writing all files\n",
      "INFO:decrypt.scrape_parse.guardian_load:Loading splits directly from given json files. Using /home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt/data/disjoint_word_init.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': -1,\n",
      " 'input': 'Suffering to grasp edge of plant (8)',\n",
      " 'target': 'agrimony'}\n",
      "{'idx': -1,\n",
      " 'input': 'Honour Ben and Noel with new order (7)',\n",
      " 'target': 'ennoble'}\n",
      "{'idx': -1, 'input': 'Bit the royal we love? Cheers! (4)', 'target': 'iota'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142380/142380 [00:00<00:00, 2223837.98it/s]\n",
      "INFO:decrypt.scrape_parse.guardian_load:Counter({1: 118540, 2: 20105, 3: 2929, 4: 686, 5: 112, 6: 8})\n",
      "INFO:decrypt.scrape_parse.guardian_load:Clue list length matches Decrypting paper expected length\n",
      "INFO:decrypt.scrape_parse.guardian_load:Got splits of lenghts [75847, 32628, 33905]\n",
      "INFO:decrypt.scrape_parse.guardian_load:First three clues of train set:\n",
      "\t[CleanGuardianClue(clue='Sailor boy in his hammock', lengths=[4], soln='abed', soln_with_spaces='abed', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Rufus', orig_lengths='4', lengths_punctuation=set()), CleanGuardianClue(clue='With a degree, I leave this subject', lengths=[5], soln='maths', soln_with_spaces='maths', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Rufus', orig_lengths='5', lengths_punctuation=set()), CleanGuardianClue(clue='Burrow to cure limb and make sure one gets up', lengths=[3, 3, 5], soln='setthealarm', soln_with_spaces='set the alarm', idx=-1, dataset='', across_or_down='', pos=(0, 0), unique_clue_id='', type='cryptic', number=0, id='', creator='Araucaria', orig_lengths='3,3,5', lengths_punctuation={','})]\n",
      "100%|██████████| 75847/75847 [00:00<00:00, 667311.43it/s]\n",
      "100%|██████████| 32628/32628 [00:00<00:00, 766517.59it/s]\n",
      "100%|██████████| 33905/33905 [00:00<00:00, 712992.99it/s]\n",
      "INFO:decrypt.common.util_data:Source target mapping:\n",
      "\tSailor boy in his hammock (4) => abed\n",
      "\n",
      "INFO:decrypt.common.util_data:Finished writing all files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': -1, 'input': 'Sailor boy in his hammock (4)', 'target': 'abed'}\n",
      "{'idx': -1,\n",
      " 'input': 'With a degree, I leave this subject (5)',\n",
      " 'target': 'maths'}\n",
      "{'idx': -1,\n",
      " 'input': 'Burrow to cure limb and make sure one gets up (3,3,5)',\n",
      " 'target': 'set the alarm'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_dataset(split_type: str, overwrite=False):\n",
    "    assert split_type in ['naive_random', 'naive_disjoint', 'word_init_disjoint']\n",
    "    if split_type == 'naive_random':\n",
    "        load_fn = load_guardian_splits\n",
    "        tgt_dir = config.DataDirs.DataExport.guardian_naive_random_split\n",
    "    elif split_type == 'naive_disjoint':\n",
    "        load_fn = load_guardian_splits_disjoint\n",
    "        tgt_dir = config.DataDirs.DataExport.guardian_naive_disjoint_split\n",
    "    else:\n",
    "        load_fn = load_guardian_splits_disjoint_hash\n",
    "        tgt_dir = config.DataDirs.DataExport.guardian_word_init_disjoint_split\n",
    "\n",
    "    _, _, (train, val, test) = load_fn(k_json_folder)\n",
    "\n",
    "    os.makedirs(tgt_dir, exist_ok=True)\n",
    "    # write the output as json\n",
    "    try:\n",
    "        clue_list_tuple_to_train_split_json((train, val, test),\n",
    "                                            comment=f'Guardian data. Split: {split_type}',\n",
    "                                            export_dir=tgt_dir,\n",
    "                                            overwrite=overwrite)\n",
    "    except FileExistsError:\n",
    "        logging.warning(f'You have already generated the {split_type} dataset.\\n'\n",
    "                        f'It is located at {tgt_dir}\\n'\n",
    "                        f'To regenerate, pass overwrite=True or delete it\\n')\n",
    "\n",
    "\n",
    "make_dataset('naive_random')\n",
    "make_dataset('word_init_disjoint')\n",
    "# you can also make_dataset('naive_disjoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 Running (training) the model\n",
    "1. Setup environment\n",
    "    1. You should setup wandb for logging (that's where metrics will show up).\n",
    "    If you try to run without wandb, then wandb will tell you what you need to do to initialize\n",
    "\n",
    "    1. The relevant libraries used for our runs are\n",
    "        - transformers==4.4.2\n",
    "        - wandb==0.10.13 # this can probably be updated\n",
    "        - torch==1.7.1+cu110\n",
    "        - torchvision==0.8.2+cu110\n",
    "    - Choose place for your wandb dir, e.g., `'./wandb' `\n",
    "1. Train the model\n",
    "    1. Note that the default arguments are given in args_cryptic. See `--default_train` and `--default_val`\n",
    "    - Note that, when looking at logging messages or wandb, it will appear that epochs start at 11.\n",
    "    This is done so that we have \"space\" for 10 \"warmup\" epochs for curricular training.\n",
    "     This space causes all plots in wandb to line up.\n",
    "    1. from directory seq2seq, run the commands in the box below.\n",
    "     This will produce model checkpoints that can then be used for evaluation.\n",
    "\n",
    "\n",
    "\n",
    "Baseline naive\n",
    "```python\n",
    "python train_clues.py --default_train=base --name=baseline_naive --project=baseline --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/naive_random'\n",
    "```\n",
    "Baseline (naive split), without lengths\n",
    "```python\n",
    "python train_clues.py --default_train=base --name=baseline_naive_nolens --project=baseline --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/word_initial_disjoint' --special=no_lens\n",
    "```\n",
    "Baseline disjoint (word initial disjoint)\n",
    "```python\n",
    "python train_clues.py --default_train=base --name=baseline_disj --project=baseline --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/word_initial_disjoint'\n",
    "```\n",
    "Baseline disjoint (word initial disjoint), without lengths\n",
    "```python\n",
    "python train_clues.py --default_train=base --name=baseline_disj --project=baseline --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/word_initial_disjoint' --special=no_lens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 Evaluating the model\n",
    "During training we generate only 5 beams for efficiency. For eval we generate 100.\n",
    "1. Select the best model based on num_match_top_sampled. There should be\n",
    "a logging statement at the end of the run that prints the location\n",
    "of the best model checkpoint.\n",
    "You can also find it by matching the peak in the wandb.ai metrics graph\n",
    "to the appropriate model save.\n",
    "2. Run eval using that model (see commands below), which will\n",
    "produce a file in a (new, different) wandb directory that looks like `epoch_11.pth.tar.preds.json` (i.e only a single epoch)\n",
    "\n",
    "For example,\n",
    "\n",
    "Baseline naive, if epoch 20 is best (you'll need to set the run_name)\n",
    "This produces generations for the validation set\n",
    "```python\n",
    "python train_clues.py --default_val=base --name=baseline_naive_val --project=baseline --data_dir='../data/clue_json/guardian/naive_random' --ckpt_path='./wandb/run_name/files/epoch_20.pth.tar\n",
    "```\n",
    "\n",
    "To produce generations for the test set,\n",
    "```python\n",
    "python train_clues.py --default_val=base --name=baseline_naive_val --project=baseline --data_dir='../data/clue_json/guardian/naive_random' --ckpt_path='./wandb/run_name/files/epoch_10.pth.tar --test\n",
    "```\n",
    "\n",
    "This should also be run for the no-lengths versions if you want to replicate those results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we produce metrics by evaluating the json that was produced\n",
    "1. Change the k_t5_outputs_dir value to the location where you have saved the json files. \n",
    "    - Recommend copying all of the preds.json files into a common directory and working from that.\n",
    "    - Alternatively you could modify the code below and pass in a full path name to each of the json outputs (using the wandb directory path)\n",
    "1. For each t5 model eval (above) that you ran (each of which produced some `..preds.json` file, run `load_and_run()` to get metrics for those outputs\n",
    "1. The resulting outputs are the values we report in the tables. See `decrypt/common/validation_tools.ModelEval` for more details about the numbers that are produced. Percentages are prefixed by agg_\n",
    "\n",
    "Note that for the Main Results Table 2, the metrics we include in the table correspond to\n",
    "- `agg_top_match`\n",
    "- `agg_top_10_after_filter`\n",
    "\n",
    "More details of these metric calculations can be found in `decrypt.common.validation_tools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'decrypt/t5_outputs/baseline_naive_e12_test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# for example, if your output files are in\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# 'decrypt/t5_outputs/'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# and you will run the below, e.g., if you have named the files\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# for example\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m### primary - test\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m vt\u001b[39m.\u001b[39;49mload_and_run_t5(\u001b[39m'\u001b[39;49m\u001b[39mdecrypt/t5_outputs/baseline_naive_e12_test\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m vt\u001b[39m.\u001b[39mload_and_run_t5(\u001b[39m'\u001b[39m\u001b[39mdecrypt/t5_outputs/baseline_naive_nolens_e15_test\u001b[39m\u001b[39m'\u001b[39m)     \u001b[39m# test set\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m## primary val\u001b[39;00m\n",
      "File \u001b[0;32m~/mbzuai/decrypting-crosswords/decrypt/decrypt/common/validation_tools.py:466\u001b[0m, in \u001b[0;36mload_and_run_t5\u001b[0;34m(fname, label, filter_fcn, pre_truncate, do_length_filter)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     label \u001b[39m=\u001b[39m fname\n\u001b[0;32m--> 466\u001b[0m data \u001b[39m=\u001b[39m load_t5(fname \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.json\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    467\u001b[0m                pre_truncate\u001b[39m=\u001b[39;49mpre_truncate)\n\u001b[1;32m    468\u001b[0m all_aggregate(data, label\u001b[39m=\u001b[39mlabel, filter_fcn\u001b[39m=\u001b[39mfilter_fcn)\n",
      "File \u001b[0;32m~/mbzuai/decrypting-crosswords/decrypt/decrypt/common/validation_tools.py:441\u001b[0m, in \u001b[0;36mload_and_run_t5.<locals>.load_t5\u001b[0;34m(json_out_file, pre_truncate)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_t5\u001b[39m(json_out_file: \u001b[39mstr\u001b[39m, pre_truncate\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(json_out_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    442\u001b[0m         json_blob \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m    444\u001b[0m     \u001b[39m# eval (and backfill)\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'decrypt/t5_outputs/baseline_naive_e12_test.json'"
     ]
    }
   ],
   "source": [
    "# for example, if your output files are in\n",
    "# 'decrypt/t5_outputs/'\n",
    "# and you will run the below, e.g., if you have named the files\n",
    "# baseline_naive_e12_test.json\n",
    "# (.json will be appended for you by the load_and_run_t5 function)\n",
    "# a better name for load_and_run is load_and_eval\n",
    "\n",
    "# for example\n",
    "### primary - test\n",
    "vt.load_and_run_t5('decrypt/t5_outputs/baseline_naive_e12_test')\n",
    "vt.load_and_run_t5('decrypt/t5_outputs/baseline_naive_nolens_e15_test')     # test set\n",
    "\n",
    "## primary val\n",
    "vt.load_and_run_t5('decrypt/t5_outputs/baseline_naive_e12_val')\n",
    "vt.load_and_run_t5('decrypt/t5_outputs/baseline_naive_nolens_e15_val')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
