{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/abdelrahman.sadallah/local/cuda-11.7/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /home/abdelrahman.sadallah/.conda/envs/nlp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c3612f9d854bb4b773a0ebaa0d73ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    return_dict=True,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)  \n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# Define PAD Token = BOS Token\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "model.config.pad_token_id = model.config.bos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"\"\"### Instruction: I will give you a cryptic crossword which is a crossword puzzle in which each clue is a word puzzle. Your task is to solve this clue, and to give five possible answers for it. The number of charachters in the answer should be same as the numbers in the parenthesis in the clue. output 5 generations. The following are few examples to help you.\\n\\n### Input:\\nDesk register taken no further than Ozzie? (7)\\n\\n### Output:\\nrolltop\\n\\n### Input:\\nWhat's missing in a fight is obscure (5,3)\"\"\"\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **encoding,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.00001,\n",
    "        generation_config=  generation_config,\n",
    "    )  \n",
    "\n",
    "answer_tokens = outputs[:, encoding.input_ids.shape[1] :]\n",
    "\n",
    "output_text = tokenizer.batch_decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absent\n",
      "shocked\n",
      "shocked\n",
      "shocked\n",
      "shocked\n",
      "shocked\n",
      "shocked\n",
      "shocked\n",
      "shocked\n"
     ]
    }
   ],
   "source": [
    "for i in output_text:\n",
    "\n",
    "    lines = i.split('\\n')\n",
    "    for i,l in enumerate(lines):\n",
    "        if l=='### Output:':\n",
    "            print(lines[i+1])\n",
    "            \n",
    "# print(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   13,    13,  2277, 29937, 10604, 29901,    13, 29874,   282,  3322,\n",
       "            13,    13,  2277, 29937, 10567, 29901,    13, 29909,   767, 29915,\n",
       "         29879,   263,   767,   363,   263, 29915,   393,   313, 29945, 29897,\n",
       "            13,    13,  2277, 29937, 10604, 29901,    13, 29874,   767,    13,\n",
       "            13,  2277, 29937, 10567, 29901,    13, 29909,   767, 29915, 29879,\n",
       "           263,   767,   363,   263, 29915,   393,   313, 29945, 29897,    13,\n",
       "            13,  2277, 29937, 10604]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
