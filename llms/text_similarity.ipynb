{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/included/chatgpt_eval_results_32627_3shots_learning.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130549it [08:38, 251.99it/s]\n",
      "  5%|▍         | 1/22 [08:38<3:01:20, 518.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/included/mistral_normal_3_few_shot.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68331it [04:47, 237.36it/s]\n",
      "  5%|▍         | 1/22 [13:26<4:42:06, 806.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     44\u001b[0m embedding_1\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(label, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m embedding_2 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(prediction, convert_to_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     46\u001b[0m sim \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mpytorch_cos_sim(embedding_1, embedding_2)\n\u001b[1;32m     47\u001b[0m cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:281\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    279\u001b[0m sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index : start_index \u001b[39m+\u001b[39m batch_size]\n\u001b[1;32m    280\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(sentences_batch)\n\u001b[0;32m--> 281\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    283\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    284\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(features)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/sentence_transformers/util.py:323\u001b[0m, in \u001b[0;36mbatch_to_device\u001b[0;34m(batch, target_device)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch[key], Tensor):\n\u001b[0;32m--> 323\u001b[0m         batch[key] \u001b[39m=\u001b[39m batch[key]\u001b[39m.\u001b[39;49mto(target_device)\n\u001b[1;32m    324\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "similarities_dict = []\n",
    "base_dir = 'outputs/included'\n",
    "\n",
    "dir =directory = os.fsencode(base_dir)\n",
    "\n",
    "\n",
    "files = os.listdir(directory)[6:]\n",
    "for file in tqdm(files):\n",
    "\n",
    "    filename = os.fsdecode(file)\n",
    "\n",
    "    if filename.endswith(\".txt\"):\n",
    "        sim = 0.0\n",
    "        cnt = 0\n",
    "\n",
    "        save_name = filename\n",
    "        filename = os.path.join(base_dir, filename)\n",
    "        print(filename)\n",
    "        lines = []\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "\n",
    "            lines = list(lines[7:])\n",
    "            # for i,line in enumerate(tqdm(lines)):\n",
    "            for i,line in tqdm(enumerate(lines)):\n",
    "                if   ('-' * 10 ) in  line:\n",
    "\n",
    "                    # ' '.join(output_file[i+3].split('|')[1].split(' ')[1:-3])\n",
    "                    line_segments = lines[i-1].split('|')\n",
    "\n",
    "                    if len(line_segments)< 2:\n",
    "                        print(line)\n",
    "                    label = ' '.join(line_segments[1].split(' ')[1:-3]).strip()\n",
    "                    prediction = line_segments[0].strip()\n",
    "\n",
    "                    ### Only consider the wrong examples\n",
    "                    if label == prediction:\n",
    "                        continue\n",
    "\n",
    "                    embedding_1= model.encode(label, convert_to_tensor=True)\n",
    "                    embedding_2 = model.encode(prediction, convert_to_tensor=True)\n",
    "                    sim += util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "                    cnt += 1\n",
    "\n",
    "        # save_key = filename.split('.txt')[0] + '_similarity.txt'\n",
    "        \n",
    "        # similarities_dict[save_key] = sim/cnt\n",
    "        similarities_dict.append({'filename': save_name, 'similarity' : sim.item()/cnt})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Compute embedding for both lists\n",
    "# embedding_1= model.encode(sentences[0], convert_to_tensor=True)\n",
    "# embedding_2 = model.encode(sentences[1], convert_to_tensor=True)\n",
    "\n",
    "# sim = util.pytorch_cos_sim(embedding_1, embedding_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_path = base_dir  + 'similarity_results.json'\n",
    "\n",
    "with open(save_results_path, 'w')as f:\n",
    "    json.dump(similarities_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'chatgpt_eval_results_32627_3shots_learning.txt',\n",
       "  'similarity': 0.26035912872548106}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/includedsimilarity_results.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_results_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
