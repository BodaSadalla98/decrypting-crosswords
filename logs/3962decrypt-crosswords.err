wandb: Currently logged in as: bodasadallah2. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in ../../wandb/wandb/run-20231011_102149-vu5cdcic
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run naive_random_Cirrucular
wandb: ‚≠êÔ∏è View project at https://wandb.ai/bodasadallah2/baseline
wandb: üöÄ View run at https://wandb.ai/bodasadallah2/baseline/runs/vu5cdcic
WARNING:common_seq.util:Logger had handlers already set WTF
..... CLEARING
/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
  0%|          | 0/28476 [00:00<?, ?it/s]/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  1%|          | 256/28476 [01:55<3:32:19,  2.22it/s]  1%|          | 256/28476 [01:55<3:32:19,  2.22it/s, NLL=9.56]                                                                 1%|          | 256/28476 [01:55<3:32:19,  2.22it/s, NLL=9.56]  1%|          | 256/28476 [02:01<3:43:05,  2.11it/s, NLL=9.56]
  0%|          | 0/22349 [00:00<?, ?it/s]  1%|          | 256/22349 [00:26<37:49,  9.73it/s]  1%|          | 256/22349 [00:26<37:49,  9.73it/s, NLL=10]  1%|          | 256/22349 [00:35<51:21,  7.17it/s, NLL=10]
  0%|          | 0/22349 [00:00<?, ?it/s]  1%|          | 256/22349 [00:17<24:45, 14.87it/s]  1%|          | 256/22349 [00:17<24:45, 14.87it/s, NLL=7.61]  1%|          | 256/22349 [00:20<30:05, 12.24it/s, NLL=7.61]
  0%|          | 0/4425122 [00:00<?, ?it/s]  0%|          | 256/4425122 [01:26<417:04:22,  2.95it/s]  0%|          | 256/4425122 [01:27<417:04:22,  2.95it/s, epoch=1, loss=8.61]  0%|          | 512/4425122 [01:28<177:12:20,  6.94it/s, epoch=1, loss=8.61]  0%|          | 512/4425122 [01:28<177:12:20,  6.94it/s, epoch=1, loss=8.57]  0%|          | 768/4425122 [01:29<97:07:21, 12.65it/s, epoch=1, loss=8.57]   0%|          | 768/4425122 [01:29<97:07:21, 12.65it/s, epoch=1, loss=8.64]  0%|          | 1024/4425122 [01:29<60:04:18, 20.46it/s, epoch=1, loss=8.64]  0%|          | 1024/4425122 [01:29<60:04:18, 20.46it/s, epoch=1, loss=7.49]  0%|          | 1280/4425122 [01:30<39:03:54, 31.46it/s, epoch=1, loss=7.49]  0%|          | 1280/4425122 [01:30<39:03:54, 31.46it/s, epoch=1, loss=6.98]  0%|          | 1536/4425122 [01:30<26:14:27, 46.83it/s, epoch=1, loss=6.98]  0%|          | 1536/4425122 [01:30<26:14:27, 46.83it/s, epoch=1, loss=7.16]  0%|          | 1792/4425122 [01:30<18:09:38, 67.66it/s, epoch=1, loss=7.16]  0%|          | 1792/4425122 [01:30<18:09:38, 67.66it/s, epoch=1, loss=8.54]  0%|          | 2048/4425122 [01:35<19:45:56, 62.16it/s, epoch=1, loss=8.54]  0%|          | 2048/4425122 [01:35<19:45:56, 62.16it/s, epoch=1, loss=8.3]   0%|          | 2304/4425122 [01:36<14:14:32, 86.26it/s, epoch=1, loss=8.3]  0%|          | 2304/4425122 [01:36<14:14:32, 86.26it/s, epoch=1, loss=8.55]  0%|          | 2560/4425122 [01:36<10:48:36, 113.64it/s, epoch=1, loss=8.55]  0%|          | 2560/4425122 [01:36<10:48:36, 113.64it/s, epoch=1, loss=7.52]  0%|          | 2816/4425122 [01:45<20:31:27, 59.85it/s, epoch=1, loss=7.52]   0%|          | 2816/4425122 [01:45<20:31:27, 59.85it/s, epoch=1, loss=6.88]  0%|          | 3072/4425122 [01:49<19:24:49, 63.27it/s, epoch=1, loss=6.88]  0%|          | 3072/4425122 [01:49<19:24:49, 63.27it/s, epoch=1, loss=6.99]  0%|          | 3328/4425122 [01:49<13:51:41, 88.61it/s, epoch=1, loss=6.99]  0%|          | 3328/4425122 [01:49<13:51:41, 88.61it/s, epoch=1, loss=8.34]  0%|          | 3584/4425122 [01:50<11:42:53, 104.84it/s, epoch=1, loss=8.34]  0%|          | 3584/4425122 [01:50<11:42:53, 104.84it/s, epoch=1, loss=8.33]  0%|          | 3840/4425122 [01:53<11:32:26, 106.42it/s, epoch=1, loss=8.33]  0%|          | 3840/4425122 [01:53<11:32:26, 106.42it/s, epoch=1, loss=8.02]  0%|          | 4096/4425122 [01:53<8:40:40, 141.51it/s, epoch=1, loss=8.02]   0%|          | 4096/4425122 [01:53<8:40:40, 141.51it/s, epoch=1, loss=7.24]  0%|          | 4352/4425122 [01:54<7:46:27, 157.95it/s, epoch=1, loss=7.24]  0%|          | 4352/4425122 [01:54<7:46:27, 157.95it/s, epoch=1, loss=6.88]  0%|          | 4352/4425122 [02:07<7:46:27, 157.95it/s, epoch=1, loss=6.88]  0%|          | 4608/4425122 [02:52<88:14:55, 13.91it/s, epoch=1, loss=6.88]  0%|          | 4608/4425122 [02:52<88:14:55, 13.91it/s, epoch=1, loss=6.68]  0%|          | 4864/4425122 [02:52<62:05:18, 19.78it/s, epoch=1, loss=6.68]  0%|          | 4864/4425122 [02:52<62:05:18, 19.78it/s, epoch=1, loss=8.32]  0%|          | 5120/4425122 [02:52<43:53:12, 27.98it/s, epoch=1, loss=8.32]  0%|          | 5120/4425122 [02:52<43:53:12, 27.98it/s, epoch=1, loss=8.12]  0%|          | 5376/4425122 [02:53<31:09:41, 39.40it/s, epoch=1, loss=8.12]  0%|          | 5376/4425122 [02:53<31:09:41, 39.40it/s, epoch=1, loss=8.14]  0%|          | 5632/4425122 [02:53<22:16:21, 55.12it/s, epoch=1, loss=8.14]  0%|          | 5632/4425122 [02:53<22:16:21, 55.12it/s, epoch=1, loss=6.78]  0%|          | 5888/4425122 [02:53<16:14:52, 75.55it/s, epoch=1, loss=6.78]  0%|          | 5888/4425122 [02:53<16:14:52, 75.55it/s, epoch=1, loss=6.57]  0%|          | 6144/4425122 [02:54<11:55:27, 102.94it/s, epoch=1, loss=6.57]  0%|          | 6144/4425122 [02:54<11:55:27, 102.94it/s, epoch=1, loss=6.81]  0%|          | 6400/4425122 [02:54<8:53:42, 137.99it/s, epoch=1, loss=6.81]   0%|          | 6400/4425122 [02:54<8:53:42, 137.99it/s, epoch=1, loss=7.67]  0%|          | 6656/4425122 [02:56<8:48:06, 139.44it/s, epoch=1, loss=7.67]  0%|          | 6656/4425122 [02:56<8:48:06, 139.44it/s, epoch=1, loss=7.27]  0%|          | 6912/4425122 [02:58<8:34:54, 143.01it/s, epoch=1, loss=7.27]  0%|          | 6912/4425122 [02:58<8:34:54, 143.01it/s, epoch=1, loss=7.63]  0%|          | 7168/4425122 [03:06<18:38:44, 65.82it/s, epoch=1, loss=7.63]  0%|          | 7168/4425122 [03:06<18:38:44, 65.82it/s, epoch=1, loss=6.28]  0%|          | 7424/4425122 [03:07<13:39:31, 89.84it/s, epoch=1, loss=6.28]  0%|          | 7424/4425122 [03:07<13:39:31, 89.84it/s, epoch=1, loss=6.42]  0%|          | 7680/4425122 [03:07<9:58:23, 123.04it/s, epoch=1, loss=6.42]  0%|          | 7680/4425122 [03:07<9:58:23, 123.04it/s, epoch=1, loss=6.38]  0%|          | 7936/4425122 [03:10<10:31:31, 116.57it/s, epoch=1, loss=6.38]  0%|          | 7936/4425122 [03:10<10:31:31, 116.57it/s, epoch=1, loss=7.35]  0%|          | 8192/4425122 [03:14<13:09:18, 93.26it/s, epoch=1, loss=7.35]   0%|          | 8192/4425122 [03:14<13:09:18, 93.26it/s, epoch=1, loss=7.34]  0%|          | 8448/4425122 [03:21<19:49:26, 61.89it/s, epoch=1, loss=7.34]  0%|          | 8448/4425122 [03:21<19:49:26, 61.89it/s, epoch=1, loss=6.94]  0%|          | 8704/4425122 [03:25<19:21:57, 63.35it/s, epoch=1, loss=6.94]  0%|          | 8704/4425122 [03:25<19:21:57, 63.35it/s, epoch=1, loss=6.1]   0%|          | 8960/4425122 [03:30<20:28:05, 59.93it/s, epoch=1, loss=6.1]  0%|          | 8960/4425122 [03:30<20:28:05, 59.93it/s, epoch=1, loss=6.06]  0%|          | 9216/4425122 [03:32<17:16:50, 70.98it/s, epoch=1, loss=6.06]  0%|          | 9216/4425122 [03:32<17:16:50, 70.98it/s, epoch=1, loss=5.95]  0%|          | 9472/4425122 [03:32<12:37:02, 97.21it/s, epoch=1, loss=5.95]  0%|          | 9472/4425122 [03:32<12:37:02, 97.21it/s, epoch=1, loss=7.05]  0%|          | 9728/4425122 [03:35<13:19:08, 92.09it/s, epoch=1, loss=7.05]  0%|          | 9728/4425122 [03:35<13:19:08, 92.09it/s, epoch=1, loss=6.86]  0%|          | 9984/4425122 [03:37<12:21:45, 99.20it/s, epoch=1, loss=6.86]  0%|          | 9984/4425122 [03:37<12:21:45, 99.20it/s, epoch=1, loss=6.49]  0%|          | 10240/4425122 [03:42<14:58:20, 81.91it/s, epoch=1, loss=6.49]  0%|          | 10240/4425122 [03:42<14:58:20, 81.91it/s, epoch=1, loss=6.09]  0%|          | 10496/4425122 [03:45<15:05:39, 81.24it/s, epoch=1, loss=6.09]  0%|          | 10496/4425122 [03:45<15:05:39, 81.24it/s, epoch=1, loss=5.8]   0%|          | 10752/4425122 [03:54<23:28:27, 52.24it/s, epoch=1, loss=5.8]  0%|          | 10752/4425122 [03:54<23:28:27, 52.24it/s, epoch=1, loss=5.74]  0%|          | 11008/4425122 [03:59<24:13:36, 50.61it/s, epoch=1, loss=5.74]  0%|          | 11008/4425122 [03:59<24:13:36, 50.61it/s, epoch=1, loss=6.39]  0%|          | 11264/4425122 [03:59<17:17:44, 70.89it/s, epoch=1, loss=6.39]  0%|          | 11264/4425122 [03:59<17:17:44, 70.89it/s, epoch=1, loss=6.29]  0%|          | 11520/4425122 [04:01<13:38:38, 89.86it/s, epoch=1, loss=6.29]  0%|          | 11520/4425122 [04:01<13:38:38, 89.86it/s, epoch=1, loss=5.81]  0%|          | 11776/4425122 [04:01<10:11:20, 120.32it/s, epoch=1, loss=5.81]  0%|          | 11776/4425122 [04:01<10:11:20, 120.32it/s, epoch=1, loss=5.52]  0%|          | 12032/4425122 [04:04<11:51:01, 103.44it/s, epoch=1, loss=5.52]  0%|          | 12032/4425122 [04:04<11:51:01, 103.44it/s, epoch=1, loss=5.52]  0%|          | 12288/4425122 [04:06<11:17:44, 108.52it/s, epoch=1, loss=5.52]  0%|          | 12288/4425122 [04:06<11:17:44, 108.52it/s, epoch=1, loss=5.5]   0%|          | 12544/4425122 [04:10<12:39:35, 96.82it/s, epoch=1, loss=5.5]   0%|          | 12544/4425122 [04:10<12:39:35, 96.82it/s, epoch=1, loss=5.88]  0%|          | 12800/4425122 [04:11<11:02:02, 111.08it/s, epoch=1, loss=5.88]  0%|          | 12800/4425122 [04:11<11:02:02, 111.08it/s, epoch=1, loss=6.04]  0%|          | 13056/4425122 [04:12<9:27:00, 129.69it/s, epoch=1, loss=6.04]   0%|          | 13056/4425122 [04:12<9:27:00, 129.69it/s, epoch=1, loss=5.89]  0%|          | 13312/4425122 [04:17<13:08:05, 93.30it/s, epoch=1, loss=5.89]  0%|          | 13312/4425122 [04:17<13:08:05, 93.30it/s, epoch=1, loss=5.21]  0%|          | 13568/4425122 [04:17<9:41:03, 126.54it/s, epoch=1, loss=5.21]  0%|          | 13568/4425122 [04:17<9:41:03, 126.54it/s, epoch=1, loss=5.5]   0%|          | 13824/4425122 [04:18<7:13:46, 169.49it/s, epoch=1, loss=5.5]  0%|          | 13824/4425122 [04:18<7:13:46, 169.49it/s, epoch=1, loss=5.18]  0%|          | 14080/4425122 [04:22<11:14:25, 109.01it/s, epoch=1, loss=5.18]  0%|          | 14080/4425122 [04:22<11:14:25, 109.01it/s, epoch=1, loss=5.56]  0%|          | 14336/4425122 [04:22<8:29:32, 144.28it/s, epoch=1, loss=5.56]   0%|          | 14336/4425122 [04:22<8:29:32, 144.28it/s, epoch=1, loss=5.32]  0%|          | 14592/4425122 [04:34<23:19:24, 52.53it/s, epoch=1, loss=5.32]  0%|          | 14592/4425122 [04:34<23:19:24, 52.53it/s, epoch=1, loss=5.29]  0%|          | 14848/4425122 [04:37<19:40:25, 62.27it/s, epoch=1, loss=5.29]  0%|          | 14848/4425122 [04:37<19:40:25, 62.27it/s, epoch=1, loss=5.13]  0%|          | 15104/4425122 [04:41<19:10:29, 63.89it/s, epoch=1, loss=5.13]  0%|          | 15104/4425122 [04:41<19:10:29, 63.89it/s, epoch=1, loss=5.1]   0%|          | 15360/4425122 [04:41<14:21:40, 85.29it/s, epoch=1, loss=5.1]  0%|          | 15360/4425122 [04:41<14:21:40, 85.29it/s, epoch=1, loss=4.95]  0%|          | 15616/4425122 [04:43<12:25:50, 98.54it/s, epoch=1, loss=4.95]  0%|          | 15616/4425122 [04:43<12:25:50, 98.54it/s, epoch=1, loss=5.52]  0%|          | 15872/4425122 [04:46<12:36:59, 97.08it/s, epoch=1, loss=5.52]  0%|          | 15872/4425122 [04:46<12:36:59, 97.08it/s, epoch=1, loss=5.06]  0%|          | 16128/4425122 [04:46<9:32:39, 128.32it/s, epoch=1, loss=5.06]  0%|          | 16128/4425122 [04:46<9:32:39, 128.32it/s, epoch=1, loss=5.03]  0%|          | 16384/4425122 [04:48<10:04:13, 121.61it/s, epoch=1, loss=5.03]  0%|          | 16384/4425122 [04:48<10:04:13, 121.61it/s, epoch=1, loss=4.79]  0%|          | 16640/4425122 [04:53<14:13:22, 86.10it/s, epoch=1, loss=4.79]   0%|          | 16640/4425122 [04:53<14:13:22, 86.10it/s, epoch=1, loss=4.78]  0%|          | 16896/4425122 [04:58<17:14:14, 71.04it/s, epoch=1, loss=4.78]  0%|          | 16896/4425122 [04:58<17:14:14, 71.04it/s, epoch=1, loss=4.93]  0%|          | 17152/4425122 [05:03<17:57:25, 68.19it/s, epoch=1, loss=4.93]  0%|          | 17152/4425122 [05:03<17:57:25, 68.19it/s, epoch=1, loss=4.85]  0%|          | 17408/4425122 [05:12<25:35:31, 47.84it/s, epoch=1, loss=4.85]  0%|          | 17408/4425122 [05:12<25:35:31, 47.84it/s, epoch=1, loss=4.95]  0%|          | 17664/4425122 [05:15<22:39:36, 54.03it/s, epoch=1, loss=4.95]  0%|          | 17664/4425122 [05:15<22:39:36, 54.03it/s, epoch=1, loss=4.86]  0%|          | 17920/4425122 [05:19<22:15:52, 54.98it/s, epoch=1, loss=4.86]  0%|          | 17920/4425122 [05:19<22:15:52, 54.98it/s, epoch=1, loss=4.54]  0%|          | 18176/4425122 [05:20<16:13:53, 75.42it/s, epoch=1, loss=4.54]  0%|          | 18176/4425122 [05:20<16:13:53, 75.42it/s, epoch=1, loss=4.55]  0%|          | 18432/4425122 [05:22<14:30:25, 84.38it/s, epoch=1, loss=4.55]  0%|          | 18432/4425122 [05:22<14:30:25, 84.38it/s, epoch=1, loss=4.47]  0%|          | 18688/4425122 [05:22<10:31:15, 116.34it/s, epoch=1, loss=4.47]  0%|          | 18688/4425122 [05:22<10:31:15, 116.34it/s, epoch=1, loss=4.89]  0%|          | 18944/4425122 [05:27<14:18:35, 85.53it/s, epoch=1, loss=4.89]   0%|          | 18944/4425122 [05:27<14:18:35, 85.53it/s, epoch=1, loss=4.75]  0%|          | 19200/4425122 [05:27<10:26:51, 117.14it/s, epoch=1, loss=4.75]  0%|          | 19200/4425122 [05:27<10:26:51, 117.14it/s, epoch=1, loss=4.5]   0%|          | 19456/4425122 [05:29<9:50:55, 124.26it/s, epoch=1, loss=4.5]   0%|          | 19456/4425122 [05:29<9:50:55, 124.26it/s, epoch=1, loss=4.29]  0%|          | 19712/4425122 [05:33<12:15:04, 99.89it/s, epoch=1, loss=4.29]  0%|          | 19712/4425122 [05:33<12:15:04, 99.89it/s, epoch=1, loss=4.36]  0%|          | 19968/4425122 [05:34<9:18:54, 131.36it/s, epoch=1, loss=4.36]  0%|          | 19968/4425122 [05:34<9:18:54, 131.36it/s, epoch=1, loss=4.31]  0%|          | 20224/4425122 [05:34<6:55:26, 176.72it/s, epoch=1, loss=4.31]  0%|          | 20224/4425122 [05:34<6:55:26, 176.72it/s, epoch=1, loss=4.63]  0%|          | 20480/4425122 [05:34<5:10:24, 236.50it/s, epoch=1, loss=4.63]  0%|          | 20480/4425122 [05:34<5:10:24, 236.50it/s, epoch=1, loss=4.51]  0%|          | 20736/4425122 [05:43<17:00:30, 71.93it/s, epoch=1, loss=4.51]  0%|          | 20736/4425122 [05:43<17:00:30, 71.93it/s, epoch=1, loss=4.53]  0%|          | 20992/4425122 [05:46<15:11:53, 80.49it/s, epoch=1, loss=4.53]  0%|          | 20992/4425122 [05:46<15:11:53, 80.49it/s, epoch=1, loss=4.23]  0%|          | 21248/4425122 [05:47<12:57:35, 94.39it/s, epoch=1, loss=4.23]  0%|          | 21248/4425122 [05:47<12:57:35, 94.39it/s, epoch=1, loss=3.91]  0%|          | 21504/4425122 [05:50<12:35:04, 97.20it/s, epoch=1, loss=3.91]  0%|          | 21504/4425122 [05:50<12:35:04, 97.20it/s, epoch=1, loss=4.11]  0%|          | 21760/4425122 [05:50<9:25:07, 129.86it/s, epoch=1, loss=4.11]  0%|          | 21760/4425122 [05:50<9:25:07, 129.86it/s, epoch=1, loss=4.49]  0%|          | 22016/4425122 [05:52<9:10:13, 133.37it/s, epoch=1, loss=4.49]  0%|          | 22016/4425122 [05:52<9:10:13, 133.37it/s, epoch=1, loss=4.36]  1%|          | 22272/4425122 [05:54<8:43:21, 140.21it/s, epoch=1, loss=4.36]  1%|          | 22272/4425122 [05:54<8:43:21, 140.21it/s, epoch=1, loss=4.51]  1%|          | 22528/4425122 [05:54<6:34:40, 185.92it/s, epoch=1, loss=4.51]  1%|          | 22528/4425122 [05:54<6:34:40, 185.92it/s, epoch=1, loss=3.85]  1%|          | 22784/4425122 [05:55<6:29:32, 188.36it/s, epoch=1, loss=3.85]  1%|          | 22784/4425122 [05:55<6:29:32, 188.36it/s, epoch=1, loss=3.75]  1%|          | 23040/4425122 [06:06<19:52:17, 61.54it/s, epoch=1, loss=3.75]  1%|          | 23040/4425122 [06:06<19:52:17, 61.54it/s, epoch=1, loss=3.86]  1%|          | 23296/4425122 [06:07<16:04:19, 76.08it/s, epoch=1, loss=3.86]  1%|          | 23296/4425122 [06:07<16:04:19, 76.08it/s, epoch=1, loss=4.27]  1%|          | 23552/4425122 [06:09<13:01:27, 93.88it/s, epoch=1, loss=4.27]  1%|          | 23552/4425122 [06:09<13:01:27, 93.88it/s, epoch=1, loss=4.23]  1%|          | 23808/4425122 [06:10<11:00:44, 111.02it/s, epoch=1, loss=4.23]  1%|          | 23808/4425122 [06:10<11:00:44, 111.02it/s, epoch=1, loss=4.2]   1%|          | 24064/4425122 [06:14<13:03:28, 93.62it/s, epoch=1, loss=4.2]   1%|          | 24064/4425122 [06:14<13:03:28, 93.62it/s, epoch=1, loss=3.89]  1%|          | 24320/4425122 [06:14<9:39:57, 126.47it/s, epoch=1, loss=3.89]  1%|          | 24320/4425122 [06:14<9:39:57, 126.47it/s, epoch=1, loss=3.81]  1%|          | 24576/4425122 [06:16<9:01:33, 135.43it/s, epoch=1, loss=3.81]  1%|          | 24576/4425122 [06:16<9:01:33, 135.43it/s, epoch=1, loss=3.98]  1%|          | 24832/4425122 [06:18<10:05:35, 121.10it/s, epoch=1, loss=3.98]  1%|          | 24832/4425122 [06:18<10:05:35, 121.10it/s, epoch=1, loss=4.16]  1%|          | 25088/4425122 [06:27<20:13:48, 60.42it/s, epoch=1, loss=4.16]   1%|          | 25088/4425122 [06:27<20:13:48, 60.42it/s, epoch=1, loss=4.25]  1%|          | 25344/4425122 [06:28<14:32:32, 84.04it/s, epoch=1, loss=4.25]  1%|          | 25344/4425122 [06:28<14:32:32, 84.04it/s, epoch=1, loss=4.28]  1%|          | 25600/4425122 [06:29<12:34:17, 97.21it/s, epoch=1, loss=4.28]  1%|          | 25600/4425122 [06:29<12:34:17, 97.21it/s, epoch=1, loss=3.73]  1%|          | 25856/4425122 [06:36<17:58:22, 67.99it/s, epoch=1, loss=3.73]  1%|          | 25856/4425122 [06:36<17:58:22, 67.99it/s, epoch=1, loss=3.52]  1%|          | 26112/4425122 [06:36<12:59:05, 94.11it/s, epoch=1, loss=3.52]  1%|          | 26112/4425122 [06:36<12:59:05, 94.11it/s, epoch=1, loss=3.72]  1%|          | 26368/4425122 [06:38<11:55:58, 102.40it/s, epoch=1, loss=3.72]  1%|          | 26368/4425122 [06:38<11:55:58, 102.40it/s, epoch=1, loss=4.05]  1%|          | 26624/4425122 [06:41<12:03:31, 101.32it/s, epoch=1, loss=4.05]  1%|          | 26624/4425122 [06:41<12:03:31, 101.32it/s, epoch=1, loss=4.18]  1%|          | 26880/4425122 [06:50<21:12:09, 57.62it/s, epoch=1, loss=4.18]   1%|          | 26880/4425122 [06:50<21:12:09, 57.62it/s, epoch=1, loss=4.09]  1%|          | 27136/4425122 [06:51<16:23:58, 74.49it/s, epoch=1, loss=4.09]  1%|          | 27136/4425122 [06:51<16:23:58, 74.49it/s, epoch=1, loss=3.71]  1%|          | 27392/4425122 [06:52<13:23:45, 91.19it/s, epoch=1, loss=3.71]  1%|          | 27392/4425122 [06:52<13:23:45, 91.19it/s, epoch=1, loss=3.8]   1%|          | 27648/4425122 [06:59<18:56:42, 64.48it/s, epoch=1, loss=3.8]  1%|          | 27648/4425122 [06:59<18:56:42, 64.48it/s, epoch=1, loss=3.61]  1%|          | 27904/4425122 [06:59<13:34:13, 90.01it/s, epoch=1, loss=3.61]  1%|          | 27904/4425122 [06:59<13:34:13, 90.01it/s, epoch=1, loss=3.93]  1%|          | 28160/4425122 [07:02<13:50:05, 88.28it/s, epoch=1, loss=3.93]  1%|          | 28160/4425122 [07:02<13:50:05, 88.28it/s, epoch=1, loss=4.02]  1%|          | 28416/4425122 [07:04<13:05:37, 93.27it/s, epoch=1, loss=4.02]  1%|          | 28416/4425122 [07:04<13:05:37, 93.27it/s, epoch=1, loss=4.04]  1%|          | 28416/4425122 [07:10<18:31:13, 65.94it/s, epoch=1, loss=4.04]
Traceback (most recent call last):
  File "/home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt/seq2seq/train_clues.py", line 161, in <module>
    local_trainer.run()
  File "/home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt/seq2seq/train_abc.py", line 352, in run
    self.train_step()
  File "/home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt/seq2seq/train_abc.py", line 983, in train_step
    loss, _ = self.model_forward(pbatch.src_ids, pbatch.src_mask, pbatch.tgt_ids)
  File "/home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/decrypt/seq2seq/train_abc.py", line 919, in model_forward
    out_dict = self.model(src_ids,
  File "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1774, in forward
    lm_logits = self.lm_head(sequence_output)
  File "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB (GPU 0; 23.65 GiB total capacity; 20.35 GiB already allocated; 172.00 MiB free; 22.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run summary:
wandb:   dev/num_match_in_sample best_in_sample
wandb: dev/num_match_top_sampled best_top_sampled
wandb: 
wandb: üöÄ View run naive_random_Cirrucular at: https://wandb.ai/bodasadallah2/baseline/runs/vu5cdcic
wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ../../wandb/wandb/run-20231011_102149-vu5cdcic/logs
