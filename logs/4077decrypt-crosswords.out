[10.12 09:20:03] [train_clues.py:132 - <module>()]	 train_clues.py --default_val=base --name=naive_word_intial_disjoint_val --wandb_dir=../../wandb --project=baseline_naive --data_dir=../data/clue_json/guardian/word_initial_disjoint --ckpt_path=../../wandb/wandb/run-20231011_211723-3sdgff0h/files/epoch_12.pth.tar
[10.12 09:20:03] [util.py:168 - set_seed()]	 Setting seed
[10.12 09:20:03] [util_checkpoint.py:65 - __init__()]	 Saver will track (metric, maximize?)
 [('dev/num_match_top_sampled', True), ('epoch', True)]
[10.12 09:20:03] [util.py:86 - get_available_devices()]	 Device: cuda:0	 GPU IDs: [0]	 machine: ws-l1-013

[10.12 09:23:27] [util_dataloader_batch.py:56 - __init__()]	 Loading cluedatasetbatched of type train
[10.12 09:23:27] [util_dataloader_batch.py:72 - __init__()]	 For dataset, found readme: 
[10.12 09:23:27] [util_dataloader_batch.py:73 - __init__()]	 ['Guardian data. Split: word_init_disjoint\n', 'Total: 142380\n', 'splits: [75847, 32628, 33905]\n', '\n', "{'idx': -1, 'input': 'Sailor boy in his hammock (4)', 'target': 'abed'}\n", "{'idx': -1,\n", " 'input': 'With a degree, I leave this subject (5)',\n", " 'target': 'maths'}\n", "{'idx': -1,\n", " 'input': 'Burrow to cure limb and make sure one gets up (3,3,5)',\n", " 'target': 'set the alarm'}\n", '\n', '\n']
[10.12 09:23:40] [util_dataloader_batch.py:197 - _get_dataloader_batched()]	 Dataset train loaded with size: 75847
[10.12 09:23:42] [util_dataloader_batch.py:34 - __post_init_check()]	 Dataloader:
	"A thousand times no!" encapsulated in this Reformer (4) => knox
[10.12 09:23:42] [util_dataloader_batch.py:174 - _get_dataloader_from_dataset()]	 Dataloader loaded from dataset
[10.12 09:23:42] [util_dataloader_batch.py:56 - __init__()]	 Loading cluedatasetbatched of type val
[10.12 09:23:44] [util_dataloader_batch.py:197 - _get_dataloader_batched()]	 Dataset val loaded with size: 32628
[10.12 09:23:45] [util_dataloader_batch.py:34 - __post_init_check()]	 Dataloader:
	Tension in an arm? Slightly (1,6) => a trifle
[10.12 09:23:45] [util_dataloader_batch.py:174 - _get_dataloader_from_dataset()]	 Dataloader loaded from dataset
[10.12 09:23:45] [train_abc.py:439 - verify_and_log_trainer_info()]	 Verifying that all metrics are OK. The outputs here are NOT from the model that was passed ifone was passed
[10.12 09:23:45] [train_abc.py:549 - val_step()]	 Evaluating at all_step 0 (epoch=10)...
[10.12 09:23:45] [train_abc.py:558 - val_step()]	 Primary eval; epoch: 10
[10.12 09:24:06] [train_abc.py:620 - validate_val_loader()]	 
 idx: -1
Source: Tension in an arm? Slightly (1,6)
 	Target: a trifle
	 Actual: ? Slightly (1,6) (1,6) (1,6) (1,6) (1,

[10.12 09:24:06] [train_abc.py:447 - verify_and_log_trainer_info()]	 Tracking metrics [('dev/num_match_top_sampled', True), ('epoch', True)] all verified
[10.12 09:24:06] [train_abc.py:477 - verify_and_log_trainer_info()]	 
total_train_steps (num_train_ex * epochs): 32628
machine: ws-l1-013
num_train: 75847
num_val: 32628ada: True
ada_constant: False
add_special_tokens: False
batch_size: 16
batched_dl: True
ckpt_path: ../../wandb/wandb/run-20231011_211723-3sdgff0h/files/epoch_12.pth.tar
comment: 
data_dir: ../data/clue_json/guardian/word_initial_disjoint
default_train: None
default_val: base
dev_run: False
do_sample: True
do_save: False
early_stopping: None
fast_tokenizer: True
generation_beams: 100
grad_accum_steps: 1
hacky: False
model_name: t5-base
multi_gpu: None
multitask: None
multitask_num: -1
name: naive_word_intial_disjoint_val
no_train: True
num_epochs: 1
num_train: 75847
num_val: 32628
num_workers: 4
project: baseline_naive
resume_train: False
seed: 42
special: None
test: False
total_train: 32628
use_json: True
val_freq: None
wandb_dir: ../../wandb
No aux config (e.g. multitask) given

[10.12 09:24:06] [train_abc.py:289 - load_from_ckpt()]	 Loading checkpoint: ../../wandb/wandb/run-20231011_211723-3sdgff0h/files/epoch_12.pth.tar
[10.12 09:24:18] [util_checkpoint.py:36 - load_ckpt()]	 Loading model from ../../wandb/wandb/run-20231011_211723-3sdgff0h/files/epoch_12.pth.tar:
['{"dev/num_exact_match_char_2": 21, "dev/num_match_in_sample": 121, "dev/num_match_top_sampled": 80, "dev/num_match_top_5_sampled": 121, "dev/pct_correct_length": 9220.999999999998, "dev/pct_correct_wordct": 30917.6, "dev/NLL": 3.6640562913837833, "epoch": 12, "name": "naive_word_intial_disjoint"}']
[10.12 09:24:18] [train_abc.py:337 - run()]	 arg no_train given. Just doing single validation. Setting to epoch == 1
[10.12 09:24:18] [train_abc.py:549 - val_step()]	 Evaluating at all_step 0 (epoch=11)...
[10.12 09:24:18] [train_abc.py:558 - val_step()]	 Primary eval; epoch: 11
[10.12 09:24:26] [train_abc.py:620 - validate_val_loader()]	 
 idx: -1
Source: Tension in an arm? Slightly (1,6)
 	Target: a trifle
	 Actual: a slight

[10.12 09:26:41] [utils.py:148 - _init_num_threads()]	 Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[10.12 09:26:41] [utils.py:160 - _init_num_threads()]	 NumExpr defaulting to 8 threads.
[10.12 10:39:35] [train_abc.py:493 - val_end_epoch()]	 dev/num_exact_match_char_2: 21.00	 avg: 0.0006
[10.12 10:39:35] [train_abc.py:493 - val_end_epoch()]	 dev/num_match_in_sample: 1466.00	 avg: 0.0449
[10.12 10:39:35] [train_abc.py:493 - val_end_epoch()]	 dev/num_match_top_sampled: 116.00	 avg: 0.0036
[10.12 10:39:35] [train_abc.py:493 - val_end_epoch()]	 dev/num_match_top_5_sampled: 467.00	 avg: 0.0143
[10.12 10:39:35] [train_abc.py:493 - val_end_epoch()]	 dev/pct_correct_length: 9423.51	 avg: 0.2888
[10.12 10:39:35] [train_abc.py:493 - val_end_epoch()]	 dev/pct_correct_wordct: 30962.83	 avg: 0.9490
[10.12 10:39:35] [train_abc.py:493 - val_end_epoch()]	 dev/NLL: 03.68	 avg: 3.6796
[10.12 10:39:35] [util_checkpoint.py:142 - save_if_best()]	 Best metric for dev/num_match_top_sampled = 116 at epoch=11
[10.12 10:39:35] [util_checkpoint.py:113 - save_most_recent()]	 Saving most recent model at epoch=11 to ../../wandb/wandb/run-20231012_092001-d05owiz5/files/epoch_11.pth.tar
[10.12 10:39:37] [util_checkpoint.py:142 - save_if_best()]	 Best metric for epoch = 11 at epoch=11
