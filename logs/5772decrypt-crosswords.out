[10.29 12:47:52] [train_clues.py:132 - <module>()]	 train_clues.py --default_val=base --name=naive_acw_val --wandb_dir=../../wandb --project=baseline_naive --data_dir=../data/clue_json/guardian/naive_random --ckpt_path=../../wandb/wandb/run-20231028_200336-23m5kqq7/files/epoch_34.pth.tar
[10.29 12:47:52] [util.py:168 - set_seed()]	 Setting seed
[10.29 12:47:52] [util_checkpoint.py:65 - __init__()]	 Saver will track (metric, maximize?)
 [('dev/num_match_top_sampled', True), ('epoch', True)]
[10.29 12:47:52] [util.py:86 - get_available_devices()]	 Device: cuda:0	 GPU IDs: [0]	 machine: ws-l4-009

[10.29 12:49:16] [util_dataloader_batch.py:56 - __init__()]	 Loading cluedatasetbatched of type train
[10.29 12:49:16] [util_dataloader_batch.py:72 - __init__()]	 For dataset, found readme: 
[10.29 12:49:16] [util_dataloader_batch.py:73 - __init__()]	 ['Guardian data. Split: naive_random\n', 'Total: 142380\n', 'splits: [85428, 28476, 28476]\n', '\n', "{'idx': -1,\n", " 'input': 'Suffering to grasp edge of plant (8)',\n", " 'target': 'agrimony'}\n", "{'idx': -1,\n", " 'input': 'Honour Ben and Noel with new order (7)',\n", " 'target': 'ennoble'}\n", "{'idx': -1, 'input': 'Bit the royal we love? Cheers! (4)', 'target': 'iota'}\n", '\n', '\n']
[10.29 12:49:22] [util_dataloader_batch.py:197 - _get_dataloader_batched()]	 Dataset train loaded with size: 85428
[10.29 12:49:24] [util_dataloader_batch.py:34 - __post_init_check()]	 Dataloader:
	Try to convert Senegal? I've failed (10) => evangelise
[10.29 12:49:24] [util_dataloader_batch.py:174 - _get_dataloader_from_dataset()]	 Dataloader loaded from dataset
[10.29 12:49:24] [util_dataloader_batch.py:56 - __init__()]	 Loading cluedatasetbatched of type val
[10.29 12:49:26] [util_dataloader_batch.py:197 - _get_dataloader_batched()]	 Dataset val loaded with size: 28476
[10.29 12:49:27] [util_dataloader_batch.py:34 - __post_init_check()]	 Dataloader:
	Desk register taken no further than Ozzie? (7) => rolltop
[10.29 12:49:27] [util_dataloader_batch.py:174 - _get_dataloader_from_dataset()]	 Dataloader loaded from dataset
[10.29 12:49:27] [train_abc.py:448 - verify_and_log_trainer_info()]	 Verifying that all metrics are OK. The outputs here are NOT from the model that was passed ifone was passed
[10.29 12:49:27] [train_abc.py:558 - val_step()]	 Evaluating at all_step 0 (epoch=10)...
[10.29 12:49:27] [train_abc.py:567 - val_step()]	 Primary eval; epoch: 10
[10.29 12:49:36] [train_abc.py:629 - validate_val_loader()]	 
 idx: -1
Source: Desk register taken no further than Ozzie? (7)
 	Target: rolltop
	 Actual: Ozzie? (7) (7) (8) (8) (8) (8) (8) 

[10.29 12:49:37] [train_abc.py:456 - verify_and_log_trainer_info()]	 Tracking metrics [('dev/num_match_top_sampled', True), ('epoch', True)] all verified
[10.29 12:49:37] [train_abc.py:486 - verify_and_log_trainer_info()]	 
total_train_steps (num_train_ex * epochs): 28476
machine: ws-l4-009
num_train: 85428
num_val: 28476ada: True
ada_constant: False
add_special_tokens: False
batch_size: 16
batched_dl: True
ckpt_path: ../../wandb/wandb/run-20231028_200336-23m5kqq7/files/epoch_34.pth.tar
comment: 
data_dir: ../data/clue_json/guardian/naive_random
default_train: None
default_val: base
dev_run: False
do_sample: True
do_save: False
early_stopping: None
fast_tokenizer: True
generation_beams: 100
grad_accum_steps: 1
hacky: False
model_name: t5-base
multi_gpu: None
multitask: None
multitask_num: -1
name: naive_acw_val
no_train: True
num_epochs: 1
num_train: 85428
num_val: 28476
num_workers: 4
project: baseline_naive
resume_train: False
seed: 42
special: None
test: False
total_train: 28476
use_json: True
val_freq: None
wandb_dir: ../../wandb
No aux config (e.g. multitask) given

[10.29 12:49:37] [train_abc.py:289 - load_from_ckpt()]	 Loading checkpoint: ../../wandb/wandb/run-20231028_200336-23m5kqq7/files/epoch_34.pth.tar
[10.29 12:49:41] [util_checkpoint.py:36 - load_ckpt()]	 Loading model from ../../wandb/wandb/run-20231028_200336-23m5kqq7/files/epoch_34.pth.tar:
['{"dev/num_exact_match_char_2": 4683, "dev/num_match_in_sample": 7893, "dev/num_match_top_sampled": 4968, "dev/num_match_top_5_sampled": 7893, "dev/pct_correct_length": 26165.599999999984, "dev/pct_correct_wordct": 27638.200000000008, "dev/NLL": 3.2493595965777007, "multi/acw/num_match_in_sample": 11055, "multi/acw/num_match_top_sampled": 7052, "multi/acw/NLL": 1.4502629256204735, "epoch": 34, "name": "naive_random_ACW"}']
[10.29 12:49:41] [train_abc.py:346 - run()]	 arg no_train given. Just doing single validation. Setting to epoch == 1
[10.29 12:49:41] [train_abc.py:558 - val_step()]	 Evaluating at all_step 0 (epoch=11)...
[10.29 12:49:41] [train_abc.py:567 - val_step()]	 Primary eval; epoch: 11
[10.29 12:49:45] [train_abc.py:629 - validate_val_loader()]	 
 idx: -1
Source: Desk register taken no further than Ozzie? (7)
 	Target: rolltop
	 Actual: trainee

[10.29 12:52:08] [utils.py:148 - _init_num_threads()]	 Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[10.29 12:52:08] [utils.py:160 - _init_num_threads()]	 NumExpr defaulting to 8 threads.
[10.29 13:50:31] [train_abc.py:502 - val_end_epoch()]	 dev/num_exact_match_char_2: 4683.00	 avg: 0.1645
[10.29 13:50:31] [train_abc.py:502 - val_end_epoch()]	 dev/num_match_in_sample: 13854.00	 avg: 0.4865
[10.29 13:50:31] [train_abc.py:502 - val_end_epoch()]	 dev/num_match_top_sampled: 4974.00	 avg: 0.1747
[10.29 13:50:31] [train_abc.py:502 - val_end_epoch()]	 dev/num_match_top_5_sampled: 8105.00	 avg: 0.2846
[10.29 13:50:31] [train_abc.py:502 - val_end_epoch()]	 dev/pct_correct_length: 25300.81	 avg: 0.8885
[10.29 13:50:31] [train_abc.py:502 - val_end_epoch()]	 dev/pct_correct_wordct: 27463.82	 avg: 0.9645
[10.29 13:50:31] [train_abc.py:502 - val_end_epoch()]	 dev/NLL: 03.26	 avg: 3.2642
[10.29 13:50:31] [util_checkpoint.py:142 - save_if_best()]	 Best metric for dev/num_match_top_sampled = 4974 at epoch=11
[10.29 13:50:31] [util_checkpoint.py:113 - save_most_recent()]	 Saving most recent model at epoch=11 to ../../wandb/wandb/run-20231029_124751-8gtnxsin/files/epoch_11.pth.tar
[10.29 13:50:33] [util_checkpoint.py:142 - save_if_best()]	 Best metric for epoch = 11 at epoch=11
