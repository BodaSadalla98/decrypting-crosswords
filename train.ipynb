{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('json', data_files=\"data/naive_random.json\", field=\"train\",split=\"train\")\n",
    "val_dataset = load_dataset('json', data_files=\"data/naive_random.json\", field=\"val\",split=\"train\")\n",
    "test_dataset = load_dataset('json', data_files=\"data/naive_random.json\", field=\"test\",split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 135430,\n",
       " 'lengths_punctuation': [','],\n",
       " 'number': 0,\n",
       " 'clue': 'Achy shaking stopped by iodine, salt and kaolin',\n",
       " 'soln': 'chinaclay',\n",
       " 'across_or_down': '',\n",
       " 'id': '',\n",
       " 'creator': 'Arachne',\n",
       " 'type': 'cryptic',\n",
       " 'unique_clue_id': '',\n",
       " 'orig_lengths': '5,4',\n",
       " 'pos': [0, 0],\n",
       " 'lengths': [5, 4],\n",
       " 'dataset': '',\n",
       " 'soln_with_spaces': 'china clay'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique types is: 1\n",
      " total number of examples: 85428,    number of unique answers: 42099\n"
     ]
    }
   ],
   "source": [
    "## Some data statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "unique_answers = np.unique(train_dataset['soln'])\n",
    "\n",
    "unique_answers = pd.DataFrame(unique_answers)\n",
    "\n",
    "\n",
    "unique_type = np.unique(train_dataset['type'])\n",
    "print(f'Total number of unique types is: {len(unique_type)}')\n",
    "\n",
    "print(f' total number of examples: {len(train_dataset)},    number of unique answers: {len(unique_answers)}')\n",
    "# unique_answers.hist()\n",
    "\n",
    "# histogram = []\n",
    "# for x in unique_answers:\n",
    "#     v = len(train_dataset.filter(lambda example: example[\"soln\"] == x))\n",
    "#     histogram.append([x, v])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat clue, with length. \n",
    "def concat_length(example):\n",
    "\n",
    "    example[\"clue\"] = f'{example[\"clue\"]} ({example[\"orig_lengths\"]})   '\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(concat_length)\n",
    "val_dataset = val_dataset.map(concat_length)\n",
    "test_dataset = test_dataset.map(concat_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.select_columns([\"clue\", \"soln\", \"soln_with_spaces\"])\n",
    "val_dataset = val_dataset.select_columns([\"clue\", \"soln\" , \"soln_with_spaces\"])\n",
    "test_dataset = test_dataset.select_columns([\"clue\", \"soln\" , \"soln_with_spaces\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clue': ['Suffering to grasp edge of plant (8)   ',\n",
       "  'Honour Ben and Noel with new order (7)   '],\n",
       " 'soln': ['agrimony', 'ennoble'],\n",
       " 'soln_with_spaces': ['agrimony', 'ennoble']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-base\"\n",
    "metric = load(\"rouge\")\n",
    "acc_metric = load(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "max_input_length = 1024\n",
    "max_target_length = 32\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"clue\"]]\n",
    "    model_inputs = tokenizer(inputs, padding= 'longest', truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"soln_with_spaces\"], padding= 'longest', truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e8901255574fb4aaa1ec127e11ccd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85428 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197f0223136746e99cc0e5531952605a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28476 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdef1d64d8d642a7a47486a796b324ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28476 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train= train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val= val_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test= test_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/abdelrahman.sadallah/local/cuda-11.7/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /home/abdelrahman.sadallah/.conda/envs/nlp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "\n",
    "##############################################################################\n",
    "    ## Calculating accuracy:\n",
    "\n",
    "    flatten_pred = np.array(predictions).flatten()\n",
    "    flatten_labels = labels.flatten()\n",
    "    accuracy_result= acc_metric.compute(references=flatten_pred, predictions=flatten_labels)\n",
    "    accuracy_result = {key: value * 100 for key, value in accuracy_result.items()}\n",
    "    result.update(accuracy_result)\n",
    "\n",
    "###############################################################################3\n",
    "\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 256\n",
    "val_batch_size = 128\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"experiments/{model_name}-finetuned-random\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=val_batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\" \",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db04ab0446d749959fa718192da57386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2037, 'learning_rate': 1.7077844311377248e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffdefce04ab4d448031b8198e0e8644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2065181732177734, 'eval_rouge1': 0.1868, 'eval_rouge2': 0.0097, 'eval_rougeL': 0.1892, 'eval_rougeLsum': 0.1877, 'eval_accuracy': 67.6942, 'eval_gen_len': 5.5279, 'eval_runtime': 55.0614, 'eval_samples_per_second': 517.168, 'eval_steps_per_second': 4.05, 'epoch': 1.5}\n",
      "{'loss': 1.25, 'learning_rate': 1.4101796407185631e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d538af5f1a24978b475aa11ef08b0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1525862216949463, 'eval_rouge1': 0.3234, 'eval_rouge2': 0.0132, 'eval_rougeL': 0.3251, 'eval_rougeLsum': 0.3233, 'eval_accuracy': 66.7794, 'eval_gen_len': 5.7106, 'eval_runtime': 61.0898, 'eval_samples_per_second': 466.133, 'eval_steps_per_second': 3.65, 'epoch': 2.99}\n",
      "{'loss': 1.2575, 'learning_rate': 1.1119760479041916e-05, 'epoch': 4.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fc669749aa49a7a7ae277485c999f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1971290111541748, 'eval_rouge1': 0.4953, 'eval_rouge2': 0.0184, 'eval_rougeL': 0.4948, 'eval_rougeLsum': 0.4944, 'eval_accuracy': 68.1005, 'eval_gen_len': 5.4426, 'eval_runtime': 59.8522, 'eval_samples_per_second': 475.772, 'eval_steps_per_second': 3.726, 'epoch': 4.49}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:1904\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1902\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss_step\n\u001b[0;32m-> 1904\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_flos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfloating_point_ops(inputs))\n\u001b[1;32m   1906\u001b[0m is_last_step_and_steps_less_than_grad_acc \u001b[39m=\u001b[39m (\n\u001b[1;32m   1907\u001b[0m     steps_in_epoch \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39mand\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m steps_in_epoch\n\u001b[1;32m   1908\u001b[0m )\n\u001b[1;32m   1910\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1911\u001b[0m     total_batched_samples \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1912\u001b[0m     \u001b[39mor\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[39m# the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m     \u001b[39m# in accelerate. So, explicitly enable sync gradients to True in that case.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:3516\u001b[0m, in \u001b[0;36mTrainer.floating_point_ops\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3504\u001b[0m \u001b[39mFor models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\u001b[39;00m\n\u001b[1;32m   3505\u001b[0m \u001b[39moperations for every backward + forward pass. If using another model, either implement such a method in the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3513\u001b[0m \u001b[39m    `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mfloating_point_ops\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 3516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfloating_point_ops(inputs)\n\u001b[1;32m   3517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:1066\u001b[0m, in \u001b[0;36mModuleUtilsMixin.floating_point_ops\u001b[0;34m(self, input_dict, exclude_embeddings)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfloating_point_ops\u001b[39m(\n\u001b[1;32m   1043\u001b[0m     \u001b[39mself\u001b[39m, input_dict: Dict[\u001b[39mstr\u001b[39m, Union[torch\u001b[39m.\u001b[39mTensor, Any]], exclude_embeddings: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   1045\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m \u001b[39m    Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[39m    batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[39m        `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m6\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_tokens(input_dict) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_parameters(exclude_embeddings\u001b[39m=\u001b[39;49mexclude_embeddings)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:990\u001b[0m, in \u001b[0;36mModuleUtilsMixin.num_parameters\u001b[0;34m(self, only_trainable, exclude_embeddings)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[39mGet number of (optionally, trainable or non-embeddings) parameters in the module.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39m    `int`: The number of parameters.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39mif\u001b[39;00m exclude_embeddings:\n\u001b[0;32m--> 990\u001b[0m     embedding_param_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    991\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.weight\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m name, module_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_modules() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module_type, nn\u001b[39m.\u001b[39mEmbedding)\n\u001b[1;32m    992\u001b[0m     ]\n\u001b[1;32m    993\u001b[0m     total_parameters \u001b[39m=\u001b[39m [\n\u001b[1;32m    994\u001b[0m         parameter \u001b[39mfor\u001b[39;00m name, parameter \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_parameters() \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m embedding_param_names\n\u001b[1;32m    995\u001b[0m     ]\n\u001b[1;32m    996\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:990\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[39mGet number of (optionally, trainable or non-embeddings) parameters in the module.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39m    `int`: The number of parameters.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39mif\u001b[39;00m exclude_embeddings:\n\u001b[0;32m--> 990\u001b[0m     embedding_param_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    991\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.weight\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m name, module_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_modules() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module_type, nn\u001b[39m.\u001b[39mEmbedding)\n\u001b[1;32m    992\u001b[0m     ]\n\u001b[1;32m    993\u001b[0m     total_parameters \u001b[39m=\u001b[39m [\n\u001b[1;32m    994\u001b[0m         parameter \u001b[39mfor\u001b[39;00m name, parameter \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_parameters() \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m embedding_param_names\n\u001b[1;32m    995\u001b[0m     ]\n\u001b[1;32m    996\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:2266\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m submodule_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m prefix \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m name\n\u001b[0;32m-> 2266\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   2267\u001b[0m     \u001b[39myield\u001b[39;00m m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:2266\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m submodule_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m prefix \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m name\n\u001b[0;32m-> 2266\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   2267\u001b[0m     \u001b[39myield\u001b[39;00m m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:2266\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m submodule_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m prefix \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m name\n\u001b[0;32m-> 2266\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   2267\u001b[0m     \u001b[39myield\u001b[39;00m m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:2262\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2260\u001b[0m     memo\u001b[39m.\u001b[39madd(\u001b[39mself\u001b[39m)\n\u001b[1;32m   2261\u001b[0m \u001b[39myield\u001b[39;00m prefix, \u001b[39mself\u001b[39m\n\u001b[0;32m-> 2262\u001b[0m \u001b[39mfor\u001b[39;00m name, module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_modules\u001b[39m.\u001b[39;49mitems():\n\u001b[1;32m   2263\u001b[0m     \u001b[39mif\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2264\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "\n",
    "model_name = \"experiments/t5-base-finetuned-random/checkpoint-1500\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clue is Suffering to grasp edge of plant (8), label is agrimony, predictions is herbaceous\n",
      " Clue is Honour Ben and Noel with new order (7), label is ennoble, predictions is benevolent\n",
      " Clue is Bit the royal we love? Cheers! (4), label is iota, predictions is sneeze\n",
      " Clue is Chemist curtailed mixture to one with a blood deficiency (8), label is ischemia, predictions is rheumatologist\n",
      " Clue is Performer's part is temporary part (7), label is artiste, predictions is emcee\n",
      " Clue is Bearded old party leader sat for artist (7), label is opposed, predictions is artist\n",
      " Clue is License prepared to help mum (7), label is silence, predictions is nanny\n",
      " Clue is Young man alien kicked on the way out (6), label is bucket, predictions is adolescent\n",
      " Clue is Omar, crazy about end of Rubaiyat, beginning to dine with Fitzgerald on sausage (10), label is mortadella, predictions is iroquoise\n",
      " Clue is Resolved to be firm (7), label is decided, predictions is resolute\n",
      " Clue is Miser books sequel to unfinished blue movie (9), label is skinflint, predictions is teddy bear apocalypse\n",
      " Clue is Go one better with music about pepper (8), label is capsicum, predictions is rooster\n",
      " Clue is Very sad to find out what's popular on Twitter (5-7), label is heart rending, predictions is astonished\n",
      " Clue is Not wholly respectable group dressed in denim mode (9), label is demimonde, predictions is agnostic\n",
      " Clue is Prim clot on march, unorthodox author (7,8), label is richmal crompton, predictions is elizabeth obama\n",
      " Clue is Dramatic work produced by testy hands? (3,7), label is the dynasts, predictions is opportunistic\n",
      " Clue is Pukka girl's identification (5), label is valid, predictions is elisabeth\n",
      " Clue is Copying the example of fans at match (9,4), label is following suit, predictions is replicating\n",
      " Clue is Sheep in main road is right little madam (6), label is argali, predictions is ielts\n",
      " Clue is American Psycho becomes less violent (6), label is abates, predictions is neophyte\n",
      " Clue is Strasbourg's punishment for yobs? (4), label is asbo, predictions is yobs\n",
      " Clue is Stamp time and time again (4), label is mint, predictions is engrave\n",
      " Clue is In inspiration, daughter shows liberality of mind (7), label is breadth, predictions is ingenuity\n",
      " Clue is Rain has force in the North (9), label is waterfall, predictions is neanderthals\n",
      " Clue is Band of cloud (4), label is blur, predictions is nirvana\n",
      " Clue is Female children move like butterflies (7), label is flitter, predictions is sphinxes\n",
      " Clue is Virgin gets garment for a pound (6), label is vestal, predictions is oxford\n",
      " Clue is Can the last of you set up a group? (4), label is unit, predictions is ottawa\n",
      " Clue is Girl doing termly revision (6), label is myrtle, predictions is naysayer\n",
      " Clue is Dog keeping busy, originally sniffing female's bottom (9), label is pekingese, predictions is rhododendron\n",
      " Clue is \"Sabre\", \"scimitar\" or \"steel\"? (5), label is sword, predictions is styrofoam\n",
      " Clue is Prescription brings about heroic recovery (6), label is recipe, predictions is resurrection\n",
      " Clue is Explosive device found on the beach (5), label is shell, predictions is explosive\n",
      " Clue is Can the German produce touchwood? (6), label is tinder, predictions is styrofoam\n",
      " Clue is Can't have venison without a bottle of wine! (8), label is decanter, predictions is oatmeal\n",
      " Clue is Cast getting hold of adult books in poor condition (6), label is shabby, predictions is styrofoam\n",
      " Clue is Totters, being legless (6), label is adders, predictions is limbless\n",
      " Clue is Yet civilians lived such a life in the war (9), label is sheltered, predictions is abolitionists\n",
      " Clue is Organ swell may affect one's hearing (6), label is earwax, predictions is eardrum\n",
      " Clue is Medicine one dispensed with odd type of disease (7), label is endemic, predictions is idiosyncrasia\n",
      " Clue is Going, going … what am I bid? (5), label is adieu, predictions is bid\n",
      " Clue is Turn buns when cooked and well browned (8), label is sunburnt, predictions is rotisserie\n",
      " Clue is A sound measure of the third man (4), label is abel, predictions is odometer\n",
      " Clue is The soldier's a good man in essence (4), label is gist, predictions is sailor\n",
      " Clue is Developing nation's first uprising (7), label is nascent, predictions is colombia\n",
      " Clue is This thus inhibiting business graduate? (7), label is embargo, predictions is aristocrat\n",
      " Clue is Lovers discussing precaution - love to proceed (about time!) (6,4), label is pillow talk, predictions is appropriate appropriate appropriate\n",
      " Clue is Vision of rook in flight (6,2,3,3), label is castle in the air, predictions is aeroplane rover\n",
      " Clue is Zorba, swimming in stream, finds bird (9), label is razorbill, predictions is ostriches\n",
      " Clue is Discovery of dirt stain causes seizure of goods (9), label is distraint, predictions is ephemeral\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 512\n",
    "\n",
    "idx = 0\n",
    "\n",
    "c = 0\n",
    "for idx in range(50):\n",
    "    clue = train_dataset[idx]['clue'] + f' ({train_dataset[idx][\"orig_lengths\"]})'\n",
    "    inputs = clue\n",
    "    inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs, num_beams=10, do_sample=True)\n",
    "    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "    label  = train_dataset[idx]['soln_with_spaces']\n",
    "\n",
    "    print(f' Clue is {clue}, label is {label}, predictions is {predicted_title}')\n",
    "\n",
    "    if label == predicted_title:\n",
    "        c +=1\n",
    "\n",
    "print(c*2) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
